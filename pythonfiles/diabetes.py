# -*- coding: utf-8 -*-
"""diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o0T_ySJxhXuNeT2CMCnuo6SK4MnAlzGD
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# loading the diabetes dataset to a pandas DataFrame
diabetes_dataset = pd.read_csv('/content/diabetes.csv')

diabetes_dataset.describe()

diabetes_dataset.isnull().sum()

diabetes_dataset.head(3)

diabetes_dataset.info()

sns.heatmap(data=diabetes_dataset.corr(),annot=True)
plt.show()

for feature in diabetes_dataset.columns:
      Q1 = diabetes_dataset[feature].quantile(0.25)
      Q3 = diabetes_dataset[feature].quantile(0.75)
      IQR = Q3 - Q1
      lower = Q1 - 1.5 * IQR
      upper = Q3 + 1.5 * IQR

      if (diabetes_dataset[feature] >= upper).any():
          print(feature, "yes")  # Outlier exists
      else:
          print(feature, "no")   # No outlier

sns.boxplot(x='Insulin',data=diabetes_dataset)

diabetes_dataset.shape

# Step 1: Calculate Q1, Q3, and IQR
Q1 = diabetes_dataset.quantile(0.25)
Q3 = diabetes_dataset.quantile(0.75)
IQR = Q3 - Q1

# Step 2: Calculate lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Step 3: Remove rows with any outlier
diabetes_dataset = diabetes_dataset[~((diabetes_dataset < lower_bound) | (diabetes_dataset > upper_bound)).any(axis=1)]
diabetes_dataset.reset_index(drop=True, inplace=True)

# Optional: Check updated shape
print("Dataset shape after removing outliers:", diabetes_dataset.shape)

plt.figure(figsize=(15,5))
sns.boxplot(x='Insulin',data=diabetes_dataset)

# Exploring Imbalance In Dataset

diabetes_dataset['Outcome'].value_counts()

# Extracting Features Into Features & Target
X = diabetes_dataset.drop(['Outcome'], axis=1)
Y = diabetes_dataset['Outcome']

print('Feature (X) Shape Before Balancing :', X.shape)
print('Target (Y) Shape Before Balancing :', Y.shape)

from imblearn.over_sampling import SMOTE


# Intialising SMOTE Object
sm = SMOTE(random_state=300)
# Resampling Data
X, Y = sm.fit_resample(X, Y)


print('Feature (X) Shape After Balancing :', X.shape)
print('Target (Y) Shape After Balancing :', Y.shape)

X = diabetes_dataset.iloc[:,:-1]
X

Y=diabetes_dataset["Outcome"]
Y

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)
X

Y=diabetes_dataset["Outcome"]
Y

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.3, random_state=900)

X_train

Y_train

from sklearn import metrics
from sklearn.metrics import classification_report

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=99)
lr.fit(X_train, Y_train)
Y_pred = lr.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
lr.score(X_train,Y_train)*100,lr.score(X_test,Y_test)*100

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the hyperparameter configuration space
logreg_params = {
      'penalty': ['l1','l2'],
      'C': [0.01, 0.1, 1, 10],
      'solver': ['liblinear'],
      'max_iter': [100, 200, 500],
                                     }
# Grid search with 3-fold cross-validation
lr_grid = GridSearchCV(lr, logreg_params, cv=3)
lr_grid.fit(X_train, Y_train)

# Best hyperparameters
print("Best Parameters:", lr_grid.best_params_)
print( lr_grid.best_estimator_)

lr_grid.fit(X_train,Y_train)
Y_pred = lr_grid.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
lr_grid.score(X_train,Y_train)*100,lr_grid.score(X_test,Y_test)*100

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train,Y_train)
Y_pred = knn.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
knn.score(X_train,Y_train)*100,knn.score(X_test,Y_test)*100

#KNN
from sklearn.model_selection import GridSearchCV
knn_params = {
    'n_neighbors': [2,5,7,12,15],
        }
# Grid search with 3-fold cross-validation
knn_grid = GridSearchCV(knn, knn_params, cv=3)

# Fit the model on training data
knn_grid.fit(X_train, Y_train)

# Best hyperparameters
print("Best Parameters:", knn_grid.best_params_)
print( knn_grid.best_estimator_)

knn_grid.fit(X_train,Y_train)
Y_pred = knn_grid.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
knn_grid.score(X_train,Y_train)*100,knn_grid.score(X_test,Y_test)*100

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,Y_train)
Y_pred = dt.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
dt.score(X_train,Y_train)*100,dt.score(X_test,Y_test)*100

from sklearn import tree
plt.figure(figsize=(15,10))
tree.plot_tree(dt,filled=True)

dt_params = {
        'criterion': ['gini', 'entropy'],
        'splitter': ['best', 'random'],
        'max_depth': [3, 5, 10, 15],
        'max_features': ['sqrt', 'log2']
                                            }




dt_grid= GridSearchCV(dt,param_grid = dt_params,cv = 3)
dt_grid.fit(X_train,Y_train)
# Best hyperparameters
print("Best Parameters:", dt_grid.best_params_)
print( dt_grid.best_estimator_)

dt_grid.fit(X_train,Y_train)
Y_pred = dt_grid.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
dt_grid.score(X_train,Y_train)*100,dt_grid.score(X_test,Y_test)*100

from sklearn import svm
svm = svm.SVC(kernel='linear')
svm.fit(X_train, Y_train)
Y_pred = svm.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
svm.score(X_train,Y_train)*100,svm.score(X_test,Y_test)*100

#SVM
from sklearn.model_selection import GridSearchCV
svm_params = {
    'C': [1,10, 100],
    "kernel":['linear','sigmoid'],

                 }
# Grid search with 3-fold cross-validation
svm_grid = GridSearchCV(svm, svm_params, cv=3)

# Fit the model on training data
svm_grid.fit(X_train, Y_train)

# Best hyperparameters
print("Best Parameters:", svm_grid.best_params_)
print( svm_grid.best_estimator_)

svm_grid.fit(X_train,Y_train)
Y_pred = svm_grid.predict(X_test)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print("-"*70)
print("Report")
print("-"*70)
print("Confusion Matrix:")
print(str(confusion_matrix))
target_names = ['without diabetes', 'with diabetes']
acc=(confusion_matrix[0][0] + confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])
print("Accuracy by confusion matrix: "+str(acc))
print("\n")
print(classification_report(Y_test, Y_pred, target_names=target_names))
print("-"*70)
svm_grid.score(X_train,Y_train)*100,svm_grid.score(X_test,Y_test)*100

X_test

svm.predict([[0.366929,-1.410899,-2.127065,-1.341558,-0.829247,0.263529,0.896565,-0.696820]])

Y_test

input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = svm_grid.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

import pickle
# Save the trained SVM model with pickle
pickle.dump(svm_grid, open('diabetes.pkl', 'wb'))